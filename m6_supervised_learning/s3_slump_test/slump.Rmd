---
title: "Linear Regression Example"
author: "Jim Harner"
date: "6/7/2019"
output:
  html_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(dplyr)
library(sparklyr)
sc <- spark_connect(master = "local")
```

## 6.3 Concrete Slump Test Regression

Load `slump.csv` into Spark with `spark_read_csv` from the local filesystem.
```{r}
slump_sdf <- spark_read_csv(sc, "slump_sdf",
                path =  "file:///home/rstudio/rspark-tutorial/data/slump.csv")
head(slump_sdf)
```

First we need to split `slump_sdf` into a training and a test Spark DataFrame.
```{r}
slump_partition <- tbl(sc, "slump_sdf") %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 2)
slump_train_sdf <- slump_partition$training
slump_test_sdf <- slump_partition$test
```

The full model is now run.
```{r}
slump_lr_full_model <- slump_partition$training %>%
  ml_linear_regression(compressive_strength ~ cement + slag + fly_ash + water
                       + sp + coarse_aggr + fine_aggr)
summary(slump_lr_full_model)
```
Notice that the model summary does not provide much useful information. We can get p-values from a `tidy` summary.
```{r}
tidy(slump_lr_full_model)
```

The performance metrics on the training data can be extracted from the `ml_model` object:
```{r}
data.frame(lambda = 0, 
           r2 = slump_lr_full_model$summary$r2,
           rmse = slump_lr_full_model$summary$root_mean_squared_error,
           mae = slump_lr_full_model$summary$mean_absolute_error)
```
However, we actually want these metrics on the test data set.

Performance metrics for regression are now obtained by getting predictions using the training data based on the full model and then using the `ml_regression_evaluator` to get specific metrics.
```{r}
slump_lr_full_predict <- ml_predict(slump_lr_full_model, slump_train_sdf)
slump_lr_metrics <-
  data.frame(lambda = 0,
             rmse = ml_regression_evaluator(slump_lr_full_predict,
                                            label_col = "compressive_strength",
                                            metric_name = "rmse"),
             mae = ml_regression_evaluator(slump_lr_full_predict,
                                           label_col = "compressive_strength",
                                           metric_name = "mae"))
slump_lr_coef <- as.data.frame(slump_lr_full_model$coefficients)
```
This is done initially for $\lambda = 0$.

The model function for the lasso with varying values of the regularization parameter $\lambda$ is defined by:
```{r}
slump_lr_model <- function(l) {
  slump_train_sdf %>%
    ml_linear_regression(compressive_strength ~ cement + slag + fly_ash +
                         water + sp + coarse_aggr + fine_aggr,
                         elastic_net_param = 1, reg_param = l)
}
```

We now calculate the `rmse` and `mae` for each of the models.
```{r}
reg_parm <- c(0.005, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1)
for(l in reg_parm) {
  slump_lr_predict <- slump_lr_model(l) %>%
    ml_predict(slump_train_sdf)
  slump_lr_metrics <- 
    data.frame(lambda = l,
               rmse = ml_regression_evaluator(slump_lr_predict,
                                              label_col = "compressive_strength",
                                              metric_name = "rmse"),
               mae = ml_regression_evaluator(slump_lr_predict,
                                             label_col = "compressive_strength",
                                             metric_name = "mae")) %>%
    rbind(slump_lr_metrics, .)
  slump_lr_coef <- 
    as.data.frame(slump_lr_model(l)$coefficients) %>%
    cbind(slump_lr_coef, .)
}
slump_lr_metrics
```

Finally, we plot the performance measures.
```{r}
library(ggplot2)
slump_lr_metrics %>%
  ggplot(aes(x = lambda)) +
  geom_point(aes(y = rmse, color = 'rmse')) +
  geom_line(aes(y = rmse, color = 'rmse')) +
  geom_point(aes(y = mae, color = 'mae')) +
  geom_line(aes(y = mae, color = 'mae')) + 
  ggtitle("Performance Metric for the Slump Regularized Models") +
  xlab("Lambda") + ylab("Performance Measure")
```

Based on the performance metrics, it is clear we want `lambda` to be small, e.g., $\lambda = 0.005 \mbox{ or } 0.01$. However, we also want parsimony.

We now get the parameter estimates as `lambda` increases.
```{r}
names(slump_lr_coef) <- as.character(rbind(c(0.0, reg_parm)))
slump_lr_coef <- t(slump_lr_coef)
slump_lr_coef
```

The lasso trace of the coefficient estimates provides a way of picking the strength of regulation.
```{r}
library(ggplot2)
as.data.frame(cbind(lambda = c(0.0, reg_parm), slump_lr_coef)) %>%
  ggplot(aes(x = lambda)) +
  geom_line(aes(y = cement, color = 'cement')) +
  geom_line(aes(y = slag, color = 'slag')) + 
  geom_line(aes(y = fly_ash, color = 'fly_ash')) + 
  geom_line(aes(y = water, color = 'water')) + 
  geom_line(aes(y = sp, color = 'sp')) + 
  geom_line(aes(y = coarse_aggr, color = 'coarse_aggr')) + 
  geom_line(aes(y = fine_aggr, color = 'fine_aggr')) +
  ggtitle("Parameter Trace for the Slump Regulated Models") +
  xlab("Lambda") + ylab("Coeff. Estimate")
```
Over the range of  $\lambda$, we have 3 features (`cement`, `fly_ash`, and `water`) with consistently non-zero coefficient estimates. Arguably, `coarse_aggr` also deviates from 0. These agree with the model we found by *ad hoc* variable selection in Section 6.1.

At this point we pick a reasonable model to run on the test Spark DataFrame based on the above criteria.
```{r}
slump_train_sdf %>%
  ml_linear_regression(compressive_strength ~ cement + fly_ash + water + coarse_aggr,
                         alpha = 1, lambda = 0.005) %>%
  ml_predict(slump_test_sdf) %>%
  ml_regression_evaluator(label_col = "compressive_strength", metric_name = "rmse")
```
The RMSE is somewhat above that obtained for the training data. Several other models could be run, e.g., removing the `coarse_aggr` feature or changing $\lambda$ to 0.01. This would suggest that we need a training data set to narrow the field of possible models, a validation data set to hone into the ``best'' model, and a test data set for the final model.

The above approach uses a fitting process that involves human curation. Generally this is a good idea during model development. However, production models should be fully automated. This can be done using `ml_train_validation_split()` (or `ml_cross_validator` for $k$-fold cross validation) with arguments: an `ml_estimator` object (possibly an `ml_pipeline`), an `estimator_param_maps` object, and an `ml_evaluator` object. The resulting train-validation-split model could then be piped into `ml_validation_metrics()` to get a data frame of performance metrics for all combinations of hyperparameters.

```{r}
spark_disconnect(sc)
```
